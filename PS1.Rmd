Import Packages
```{r}
# Installing the required packages
library(numDeriv)
library(glmnet)
```


Problem A:
```{r}
# Defining the Gradient Descent function
gradientDescent <- function(f, starting_value, max_iter, gamma = 0.01) {
  current_value <- starting_value
  history <- numeric(max_iter) # To store function values for plotting
  
  for (i in 1:max_iter) {
    grad <- grad(f, current_value) # Compute the gradient at current value
    current_value <- current_value - gamma * grad # Update the current value
    history[i] <- f(current_value) # Store function value after update for plotting

    # Optional: Implement convergence criteria to break the loop early
    # if (norm(grad, type="2") < tolerance) {
    #    break
    # }
  }
  
  list(minimizer=current_value, history=history)
}

# Test functions
# Define the test function as per the formula given
test_function <- function(v) {
  x <- v[1]
  y <- v[2]
term1 <- (1 + (x + y + 1)^2 * (19 - 14*x + 3*x^2 - 14*y + 6*x*y + 3*y^2))
 term2 <- (30 + (2*x - 3*y)^2 * (18 - 32*x + 12*x^2 + 48*y - 36*x*y + 27*y^2))
  
  f_value <- term1 * term2
  return(f_value)
}

starting_value <- c(0, 0) # Starting at the origin
max_iter <- 1000 # Maximal number of iterations
gamma <- 0.00001 # Learning rate, might need adjustment based on the function characteristics

result <- gradientDescent(test_function, starting_value, max_iter, gamma)

print(paste("Approximate minimizer found at x =", result$minimizer[1], "y =", result$minimizer[2]))

# Plotting the path of function values
plot(result$history, type='l', main="Function Value Over Iterations", xlab="Iteration", ylab="Function Value",
     col="blue")
```


```{r}
library(numDeriv)

gradient_descent <- function(f, start_value, max_iterations, learning_rate) {
  x <- start_value
  path <- numeric(max_iterations)
  
  for (i in 1:max_iterations) {
    gradient <- grad(f, x)
    x <- x - learning_rate * gradient
    path[i] <- f(x)
  }
  
  return(list(minimizer = x, values_path = path))
}

# Example usage
# Define a test function, e.g., the Rosenbrock function
rosenbrock <- function(x) {
  sum(100 * (x[2:length(x)] - x[1:(length(x) - 1)]^2)^2 + (1 - x[1:(length(x) - 1)])^2)
}

# Set parameters
start_value <- c(0, 0)  # Initial guess
max_iterations <- 1000   # Maximum number of iterations
learning_rate <- 0.009  # Experiment with different choices

# Run gradient descent
result <- gradient_descent(rosenbrock, start_value, max_iterations, learning_rate)

# Print the minimizer and the path of function values
cat("Minimizer:", result$minimizer, "\n")
#cat("Function values path:", result$values_path, "\n")

# Plot the path of function values
plot(result$values_path, type = 'l', xlab = 'Iteration', ylab = 'Function Value',
     main = 'Gradient Descent Path')


```
```{r}
# Define the Goldstein–Price function
goldstein_price <- function(x) {
  term1 <- 1 + (x[1] + x[2] + 1)^2 * (19 - 14 * x[1] + 3 * x[1]^2 - 14 * x[2] + 6 * x[1] * x[2] + 3 * x[2]^2)
  term2 <- 30 + (2 * x[1] - 3 * x[2])^2 * (18 - 32 * x[1] + 12 * x[1]^2 + 48 * x[2] - 36 * x[1] * x[2] + 27 * x[2]^2)
  return(term1 * term2)
}

# Set parameters
start_value <- c(0, 0)  # Initial guess
max_iterations <- 1000   # Maximum number of iterations
learning_rate <- 0.000595  # Experiment with different choices

# Run gradient descent
# Using optim function for optimization
result_optim <- optim(par = start_value, fn = goldstein_price, method = "L-BFGS-B")
result <- gradient_descent(goldstein_price, start_value, max_iterations, learning_rate)

# Print the minimizer and the path of function values
cat("Minimizer:", result$minimizer, "\n")
#cat("Function values path:", result$values_path, "\n")

# Plot the path of function values
plot(result$values_path, type = 'l', xlab = 'Iteration', ylab = 'Function Value',
     main = 'Gradient Descent Path - Goldstein–Price Function')


```
```{r}
# Define the Three-Hump Camel function
three_hump_camel <- function(x) {
  return(2 * x[1]^2 - 1.05 * x[1]^4 + x[1]^6 / 6 + x[1] * x[2] + x[2]^2)
}

# Set parameters
start_value <- c(4, 4)  # Initial guess
max_iterations <- 1000   # Maximum number of iterations
learning_rate <- 0.005    # Experiment with different choices

# Run gradient descent
result <- gradient_descent(three_hump_camel, start_value, max_iterations, learning_rate)

# Print the minimizer and the path of function values
cat("Minimizer:", result$minimizer, "\n")
cat("Function values path:", result$values_path, "\n")

# Plot the path of function values
plot(result$values_path, type = 'l', xlab = 'Iteration', ylab = 'Function Value',
     main = 'Gradient Descent Path - Three-Hump Camel Function')
```







Problem B
```{r}
ridgeSGD <- function(X, y, lambda = 0.1, gamma_init = 0.1, max_iter = 1000, tol = 1e-6) {
  n <- nrow(X)
  p <- ncol(X)
  
  # Initialize coefficients (including b0 as the intercept)
  b <- rep(0, p + 1)
  X <- cbind(1, X) # Add intercept column
  
  # Iteration counter
  m <- 1
  
   # History of cost function values for convergence check (optional)
  cost_history <- numeric(max_iter)
  
  while (m <= max_iter) {
    # Choose a random sample
    i <- sample(n, 1)
    
    # Predicted value for chosen sample
    y_pred <- sum(X[i, ] * b)
    
    # Error for the chosen sample
    error <- y_pred - y[i]
    
    # Calculate the gradient for the chosen sample
    # Note: Gradient for Ridge includes 2*lambda*b[j] term for j > 0 (j = 0 is the intercept)
    gradient <- numeric(length(b))
    for (j in 1:length(b)) {
      if (j == 1) { # Intercept, no regularization
        gradient[j] <- 2 * error * X[i, j]
      } else { # Regularized coefficients
        gradient[j] <- 2 * error * X[i, j] + 2 * lambda * b[j]
      }
    }
    
    # Adaptive learning rate
    gamma <- gamma_init / sqrt(m)
    
    # Update coefficients
    b <- b - gamma * gradient
    
    # Update cost (for optional convergence check)
    y_pred_all <- X %*% b
    cost_history[m] <- mean((y - y_pred_all)^2) + lambda * sum(b[-1]^2) # Skip the intercept in regularization term
    
    # Check for convergence (Optional: Just to illustrate; here we simply use a maximum iteration count)
    if (m > 1 && abs(cost_history[m] - cost_history[m-1]) < tol) {
      message("Convergence achieved after ", m, " iterations.")
      break
    }
    
    # Increment iteration counter
    m <- m + 1
  }
  
  # Return results
  list(coefficients = b, cost_history = cost_history[1:m])
}

# Example Usage
# Let's simulate some data for illustrative purposes
set.seed(123)
n <- 100 # Number of observations
p <- 5 # Number of features
X <- matrix(rnorm(n * p), n, p)
beta_true <- c(1.5, -2, 0.5, 3, -1.5, 0.8) # True coefficients, including intercept
y <- cbind(1, X) %*% beta_true + rnorm(n) # Generate target variable

# Apply the ridgeSGD function
result <- ridgeSGD(X, y, lambda = 0.1, gamma_init = 0.1, max_iter = 1000, tol = 1e-6)

# View the estimated coefficients
print(result$coefficients)

# Compare with lm() using Ridge penalty through glmnet, if available
if ("glmnet" %in% rownames(installed.packages())) {
  
  # Note: glmnet uses alpha=0 for Ridge; here lambda is the regularization parameter
  ridge_model_glmnet <- glmnet(as.matrix(X), y, alpha = 0, lambda = 0.1)
  
  # Extract coefficients for comparison (Note: glmnet intercept is separate)
  coef_ridge_glmnet <- as.vector(coef(ridge_model_glmnet, s = 0.1))
  
  cat("Coefficients from glmnet Ridge regression:\n")
  print(coef_ridge_glmnet)
} else {
  cat("glmnet not installed. Skipping comparison with glmnet output.\n")
}

# Note: The above comparison assumes that the same lambda (regularization parameter) value is used.
# Differences in results can occur due to different optimization methods, convergence criteria, etc.
```

```{r}
ridge_estimator <- function(y, X, initial_values, max_iterations, lambda, tol = 1e-6) {
  n <- nrow(X)
  p <- ncol(X)
  a <- initial_values
  
  for (m in 1:max_iterations) {
    # Sample an index i at random from {1, ..., n}
    i <- sample(1:n, 1)
    
    # Compute the gradient of gi at a
    gi_gradient <- compute_gradient(X[i, ], y[i], a, lambda)
    
    # Update a using stochastic gradient descent
    gamma_m <- 1 / m
    a_new <- a - gamma_m * gi_gradient
    
    # Check for convergence
    if (sum((a_new - a)^2) < tol) {
      message("Convergence achieved after ", m, " iterations.")
      break
    }
    
    # Update coefficients
    a <- a_new
  }
  
  return(a)
}

compute_gradient <- function(xi, yi, a, lambda) {
  # Compute the gradient of gi at a
  residual <- yi - sum(a * xi)
  gradient <- -2 * xi * residual
  gradient[2:length(gradient)] <- gradient[2:length(gradient)] + 2 * lambda * a[2:length(a)]
  
  return(gradient)
}

# Example usage:
set.seed(123)  # Set seed for reproducibility
n <- 100
p <- 5
X <- matrix(rnorm(n * p), n, p)
beta_true <- c(2, 1.5, -1, 0.5, -2)
y <- X %*% beta_true + rnorm(n)

initial_values <- rep(0, p)
max_iterations <- 1000
lambda <- 0.1

ridge_result <- ridge_estimator(y, X, initial_values, max_iterations, lambda)
print("Ridge Estimator:")
print(ridge_result)

# Compare with OLS estimator
ols_result <- lm(y ~ X - 1)
print("OLS Estimator:")
print(coef(ols_result))
```
```{r}
ridge_estimator <- function(y, X, initial_values, max_iterations, lambda) {
  n <- nrow(X)
  p <- ncol(X)
  a <- initial_values
  
  for (m in 1:max_iterations) {
    # Sample an index i at random from {1, ..., n}
    i <- sample(1:n, 1)
    
    # Compute the gradient of gi at a
    gi_gradient <- compute_gradient(X[i, ], y[i], a, lambda)
    
    # Update a using stochastic gradient descent
    gamma_m <- 1 / m
    a <- a - gamma_m * gi_gradient
  }
  
  return(a)
}

compute_gradient <- function(xi, yi, a, lambda) {
  # Compute the gradient of gi at a
  residual <- yi - sum(a * xi)
  gradient <- -2 * xi * residual
  gradient[2:length(gradient)] <- gradient[2:length(gradient)] + 2 * lambda * a[2:length(a)]
  
  return(gradient)
}

# Example usage:
set.seed(123)  # Set seed for reproducibility
n <- 100
p <- 5
X <- matrix(rnorm(n * p), n, p)
beta_true <- c(2, 1.5, -1, 0.5, -2)
y <- X %*% beta_true + rnorm(n)

initial_values <- rep(0, p)
max_iterations <- 1000
lambda <- 0.1

ridge_result <- ridge_estimator(y, X, initial_values, max_iterations, lambda)
print("Ridge Estimator:")
print(ridge_result)

# Compare with OLS estimator
ols_result <- lm(y ~ X - 1)
print("OLS Estimator:")
print(coef(ols_result))
```

