---
output:
  html_document: default
  pdf_document: default
---
Created by: 
Marco Hafid - 22-620-546
Matus Kubla - 23-604-382

Describe the dataset and the classification problem:
We would like to create models that can predict if a person suffered from heart failure or not, for this we are using a dataset with 11 predictor variables (+ the target variable) and 918 observations.
We will use 4 different classification models -LDA, QDA, Naive Bayes and logistic- and compare their performance.

The initial dataset contains the following variables:

"Age: age of the patient [years]
Sex: sex of the patient [M: Male, F: Female]
ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
RestingBP: resting blood pressure [mm Hg]
Cholesterol: serum cholesterol [mm/dl]
FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]
ExerciseAngina: exercise-induced angina [Y: Yes, N: No]
Oldpeak: oldpeak = ST [Numeric value measured in depression]
ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
HeartDisease: output class [1: heart disease, 0: Normal]"
(source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)



Importing the libraries
```{r}
library(dplyr)
library(e1071)
library(corrplot)
library(randomForest)
library(caret)
```



Importing data
```{r}
# Set the working directory and import the dataset
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
heart_failure <- read.csv("heart.csv")
# Show a summary of the variables and print the first few rows of the dataframe
summary(heart_failure)
print(heart_failure)
```

There are some issues with the data, such as 172 observations having a cholesterol value of 0(NOTE TO MATUS: correct me if im wrong, not good in biology LOL), which is not possible, these will be removed. Also some of the categorical variables need to be transformed from text to categories.

The categories were transformed to the following:
Sex: sex of the patient [M: 0, F: 1]
ChestPainType: chest pain type [TA: 0, ATA: 1, NAP:2 , ASY: 3]
ExerciseAngina: exercise-induced angina [Y: 1, N: 0]
ST_Slope: the slope of the peak exercise ST segment [Up: 0, Flat: 1, Down: 2]
RestingECG: resting electrocardiogram results [Normal: 0, ST: 1, LVH: 2

Feature engineering and cleaning the data
```{r}
# Remove the observations where the cholesterol level is exactly 0
heart_failure <- heart_failure[heart_failure$Cholesterol != 0, ]

# Mapping for 'ChestPainType'
chest_pain_mapping <- c("TA" = 0, "ATA" = 1, "NAP" = 2, "ASY" = 3)

# Mapping for 'ST_Slope'
slope_mapping <- c("Up" = 0, "Flat" = 1, "Down" = 2)

# Mapping for 'RestingECG'
resting_ecg_mapping <- c("Normal" = 0, "ST" = 1, "LVH" = 2)

# Apply transformations to the dataframe
heart_failure <- heart_failure %>%
  mutate(Sex = factor(Sex, levels = c("M", "F"), labels = c(0, 1))) %>%
  mutate(ChestPainType = factor(ChestPainType, levels = names(chest_pain_mapping), labels = chest_pain_mapping)) %>%
  mutate(ExerciseAngina = factor(ExerciseAngina, levels = c("N", "Y"), labels = c(0, 1))) %>%
  mutate(ST_Slope = factor(ST_Slope, levels = names(slope_mapping), labels = slope_mapping)) %>%
  mutate(RestingECG = factor(RestingECG, levels = names(resting_ecg_mapping), labels = resting_ecg_mapping))

```

In the following steps we are investigating come characteristics of the variables, such as correlation or their significance in a random forest model to decide which variables to keep or discard for the classification models.

The correlation matrix shows some relatively strong correlation around 0.6 and -0.6 values, however we set our threshold at an absolute value of 0.8, which was not breached, therefore no variable was removed.
```{r}
# Creating a correlation matrix
numeric_columns <- sapply(heart_failure, is.numeric)
correlation_matrix <- cor(heart_failure[,numeric_columns])
correlation_matrix

# Plotting the correlation matrix 
corrplot(correlation_matrix, method = "color")
```

We created a Random Forest model to determine which variables to keep and which ones to discard. We calculated and plotted the variable importance values, where we decided to discard FastingBS, RestingECG and Sex due to these variables not being too significant. This leaves our final dataset with 8 predictor variables and 746 observations.
```{r}
# Create the Random Forest model
rf_model <- randomForest(HeartDisease ~ ., data = heart_failure, ntree = 100, mtry=4, type = "classification")

# Print variable importance
print("Variable Importance:")
print(round(importance(rf_model), 2))

# Plot variable importance
varImpPlot(rf_model, main = "Variable Importance")
# Remove the variables not deemed significant by Random Forest
heart_failure <- heart_failure[, !colnames(heart_failure) %in% c("Sex", "FastingBS", "RestingECG")]
```

Summary and Print the first 10 rows our final dataset  used for the classification models
```{r}
# Summarise and print the final df
summary(heart_failure)
print(head(heart_failure, 10))
```



Boxplots of the data
```{r}

```


We randomly split the data into a training and test subset with a 80:20 ratio.
```{r}
set.seed(123)

# Generate random indices for splitting
indices <- sample(seq_len(nrow(heart_failure)), 0.8 * nrow(heart_failure))

# Create training and test sets for features (X)
X_train <- heart_failure[indices, -which(names(heart_failure) == "HeartDisease")]
X_test <- heart_failure[-indices, -which(names(heart_failure) == "HeartDisease")]

# Create training and test sets for the target variable (Y)
Y_train <- heart_failure$HeartDisease[indices]
Y_test <- heart_failure$HeartDisease[-indices]
```




LDA
```{r}

```


QDA
```{r}

```



Naive-Bayes -Marco
```{r}
# Create a formula to include all variables except the target variable
formula_all_vars <- as.formula(paste("HeartDisease ~", paste(names(heart_failure)[-which(names(heart_failure) == "HeartDisease")], collapse = "+")))

# Fit the Naive Bayes model using all variables
fit_NB <- naiveBayes(formula_all_vars, usekernel = TRUE, data = heart_failure)


```

```{r}
actual <- heart_failure$HeartDisease

# Make predictions using the fitted Naive Bayes model
predicted <- predict(fit_NB, newdata = heart_failure)

# Ensure factor levels are consistent
actual <- factor(actual, levels = levels(predicted))

# Create confusion matrix
conf_matrix <- confusionMatrix(predicted, actual)

# Print the confusion matrix
print(conf_matrix)
```

```{r}
# Set a seed for reproducibility
set.seed(123)

# Assuming 'heart_failure' is your data frame
# Assuming 'HeartDisease' is the actual target variable in your dataset
in_train <- createDataPartition(heart_failure$HeartDisease, p = 0.8, list = FALSE)

# Create training and testing sets
train_data <- heart_failure[in_train, ]
test_data <- heart_failure[-in_train, ]

# Get unique levels from the complete dataset
all_levels <- levels(heart_failure$HeartDisease)

# Ensure factor levels are consistent in the training set
train_data$HeartDisease <- factor(train_data$HeartDisease, levels = all_levels)

# Fit the Naive Bayes model using the training set
fit_NB <- naiveBayes(formula_all_vars, usekernel = TRUE, data = train_data)

# Make predictions on the testing set
predicted <- predict(fit_NB, newdata = test_data)

# Ensure factor levels are consistent in the predicted values
predicted <- factor(predicted, levels = all_levels)

# Create confusion matrix
conf_matrix <- confusionMatrix(predicted, test_data$HeartDisease)

# Print the confusion matrix
print(conf_matrix)
```



Logistic -Marco
```{r}

```



-Conclusion/summary text-
